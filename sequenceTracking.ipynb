{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from fastdtw import fastdtw\n",
    "from scipy.spatial.distance import euclidean\n",
    "import time\n",
    "import pyautogui\n",
    "import subprocess\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "import numpy as np\n",
    "from fastdtw import fastdtw\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gesture_actions import GESTURE_ACTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üü¢ 'Start' frame matched for 'seek_forward'!\n",
      "üü¢ 'Mid1' frame matched for 'seek_forward'!\n",
      "üü¢ 'Mid2' frame matched for 'seek_forward'!\n",
      "‚úÖ Detected Sign: seek_forward\n",
      "üü¢ 'Start' frame matched for 'seek_forward'!\n",
      "üü¢ 'Mid1' frame matched for 'seek_forward'!\n",
      "üü¢ 'Mid2' frame matched for 'seek_forward'!\n",
      "‚úÖ Detected Sign: seek_forward\n",
      "üü¢ 'Start' frame matched for 'seek_forward'!\n",
      "üü¢ 'Mid1' frame matched for 'seek_forward'!\n",
      "üü¢ 'Mid2' frame matched for 'seek_forward'!\n",
      "‚úÖ Detected Sign: seek_forward\n",
      "üü¢ 'Start' frame matched for 'seek_forward'!\n",
      "üü¢ 'Mid1' frame matched for 'seek_forward'!\n",
      "üü¢ 'Mid2' frame matched for 'seek_forward'!\n",
      "‚úÖ Detected Sign: seek_forward\n"
     ]
    }
   ],
   "source": [
    "# Initialize Mediapipe\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "TARGET_WIDTH = 320\n",
    "TARGET_HEIGHT = 240\n",
    "cap = cv2.VideoCapture(2)\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7)\n",
    "\n",
    "# File to store gestures\n",
    "GESTURE_FILE = \"gestures.json\"\n",
    "COOLDOWN_TIME = 1  # Cooldown time before detecting the next gesture\n",
    "\n",
    "# Define key landmarks to check\n",
    "keypoints_to_check = [0, 1, 4, 5, 8, 9, 12, 13, 16, 17, 20]\n",
    " \n",
    "\n",
    "# Load existing gestures if available\n",
    "if os.path.exists(GESTURE_FILE):\n",
    "    with open(GESTURE_FILE, \"r\") as f:\n",
    "        gesture_dict = json.load(f)\n",
    "        # Convert each stored frame into a numpy array\n",
    "        gesture_dict = {\n",
    "            k: {\n",
    "                \"start\": np.array(v[\"start\"], dtype=np.float32),\n",
    "                \"mid1\": np.array(v[\"mid1\"], dtype=np.float32),\n",
    "                \"mid2\": np.array(v[\"mid2\"], dtype=np.float32),\n",
    "                \"end\": np.array(v[\"end\"], dtype=np.float32)\n",
    "            }\n",
    "            for k, v in gesture_dict.items()\n",
    "        }\n",
    "else:\n",
    "    gesture_dict = {}\n",
    "\n",
    "# Variables to track sequential matching\n",
    "pending_gesture = None  # The gesture name matched at \"start\"\n",
    "frame_stage = 0         # 0: start, 1: mid1, 2: mid2, 3: end\n",
    "last_detection_time = 0\n",
    "gesture_keyframes={}\n",
    "\n",
    "# New: Stage timeout (in seconds)\n",
    "STAGE_TIMEOUT = 5\n",
    "stage_start_time = None  # Record when the current stage started\n",
    "\n",
    "def normalize_landmarks(landmarks):\n",
    "    \"\"\"Normalize landmarks by centering and scaling relative to the entire hand size.\"\"\"\n",
    "    min_x, min_y, _ = np.min(landmarks, axis=0)\n",
    "    max_x, max_y, _ = np.max(landmarks, axis=0)\n",
    "    center_x = (min_x + max_x) / 2\n",
    "    center_y = (min_y + max_y) / 2\n",
    "    centered_landmarks = landmarks - np.array([center_x, center_y, 0])\n",
    "    hand_width, hand_height = max_x - min_x, max_y - min_y\n",
    "    scale = max(hand_width, hand_height)\n",
    "    if scale > 0:\n",
    "        centered_landmarks /= scale\n",
    "    return centered_landmarks\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.resize(frame, (TARGET_WIDTH, TARGET_HEIGHT))\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(rgb_frame)\n",
    "    current_time = time.time()\n",
    "\n",
    "    # Optional cooldown: skip processing if too soon after last detection\n",
    "    if current_time - last_detection_time < COOLDOWN_TIME:\n",
    "        cv2.imshow(\"Sign Prediction\", frame)\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q') or key == 27:\n",
    "            break\n",
    "        continue\n",
    "\n",
    "    # If we're in a sequence (frame_stage > 0) but too much time has passed, reset sequence.\n",
    "    if frame_stage > 0 and stage_start_time is not None:\n",
    "        if current_time - stage_start_time > STAGE_TIMEOUT:\n",
    "            print(\"‚è∞ Stage timed out. Resetting sequence.\")\n",
    "            pending_gesture = None\n",
    "            frame_stage = 0\n",
    "            stage_start_time = None\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            landmarks = np.array([(lm.x, lm.y, lm.z) for lm in hand_landmarks.landmark],\n",
    "                                 dtype=np.float32)\n",
    "            if landmarks.shape[0] < max(keypoints_to_check):\n",
    "                continue  # Incomplete detection; skip this frame\n",
    "\n",
    "            normalized_landmarks = normalize_landmarks(landmarks)\n",
    "            normalized_keypoints = normalized_landmarks[keypoints_to_check]\n",
    "            frame_sequence = [\"start\", \"mid1\", \"mid2\", \"end\"]\n",
    "\n",
    "            # For the first stage, if no sequence is started, try to match \"start\"\n",
    "            if frame_stage == 0:\n",
    "                for gesture_name, frames in gesture_dict.items():\n",
    "                    keyframe_points = frames[\"start\"].reshape(-1, 3)\n",
    "                    if keyframe_points.shape != normalized_keypoints.shape:\n",
    "                        continue\n",
    "                    distance, _ = fastdtw(keyframe_points, normalized_keypoints, dist=euclidean)\n",
    "                    if distance < 1:\n",
    "                        pending_gesture = gesture_name\n",
    "                        frame_stage = 1\n",
    "                        stage_start_time = current_time  # Start timing this stage\n",
    "                        print(f\"üü¢ 'Start' frame matched for '{gesture_name}'!\")\n",
    "                        break\n",
    "\n",
    "            # For subsequent stages, use the pending gesture's corresponding frame\n",
    "            elif pending_gesture:\n",
    "                stage_name = frame_sequence[frame_stage]\n",
    "                keyframe_points = gesture_dict[pending_gesture][stage_name].reshape(-1, 3)\n",
    "                if keyframe_points.shape == normalized_keypoints.shape:\n",
    "                    distance, _ = fastdtw(keyframe_points, normalized_keypoints, dist=euclidean)\n",
    "                    if distance < 1:\n",
    "                        print(f\"üü¢ '{stage_name.capitalize()}' frame matched for '{pending_gesture}'!\")\n",
    "                        frame_stage += 1\n",
    "                        stage_start_time = current_time  # Reset timer for next stage\n",
    "                        if frame_stage == 3:  # All stages matched\n",
    "                            print(f\"‚úÖ Detected Sign: {pending_gesture}\")\n",
    "                            try:\n",
    "                                GESTURE_ACTIONS[pending_gesture]()\n",
    "                            except Exception as e:\n",
    "                                print(f\"‚ùå Error executing action: {e}\")\n",
    "                            last_detection_time = current_time\n",
    "                            pending_gesture = None\n",
    "                            frame_stage = 0  # Reset for next gesture detection\n",
    "                            stage_start_time = None\n",
    "                            break  # Stop processing further for this hand\n",
    "\n",
    "    # Handle key presses to record frames or quit\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('1'):\n",
    "        if results.multi_hand_landmarks:\n",
    "            gesture_keyframes[\"start\"] = normalize_landmarks(landmarks)[keypoints_to_check].copy()\n",
    "            print(\"‚úÖ Start frame recorded!\")\n",
    "    elif key == ord('2'):\n",
    "        if results.multi_hand_landmarks:\n",
    "            gesture_keyframes[\"mid1\"] = normalize_landmarks(landmarks)[keypoints_to_check].copy()\n",
    "            print(\"‚úÖ Mid1 frame recorded!\")\n",
    "    elif key == ord('3'):\n",
    "        if results.multi_hand_landmarks:\n",
    "            gesture_keyframes[\"mid2\"] = normalize_landmarks(landmarks)[keypoints_to_check].copy()\n",
    "            print(\"‚úÖ Mid2 frame recorded!\")\n",
    "    elif key == ord('4'):\n",
    "        if results.multi_hand_landmarks:\n",
    "            gesture_keyframes[\"end\"] = normalize_landmarks(landmarks)[keypoints_to_check].copy()\n",
    "            print(\"‚úÖ End frame recorded!\")\n",
    "            gesture_name = input(\"Enter the word for this sign: \")\n",
    "            gesture_dict[gesture_name] = gesture_keyframes.copy()\n",
    "            with open(GESTURE_FILE, \"w\") as f:\n",
    "                json.dump(\n",
    "                    {k: {frame: v.tolist() for frame, v in frames.items()} for k, frames in gesture_dict.items()},\n",
    "                    f\n",
    "                )\n",
    "            print(f\"üìÅ Sign '{gesture_name}' saved permanently!\")\n",
    "    elif key == ord('q') or key == 27:\n",
    "        break\n",
    "\n",
    "    cv2.imshow(\"Sign Prediction\", frame)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import customtkinter as ctk\n",
    "import subprocess\n",
    "import pyautogui\n",
    "import json\n",
    "from tkinter import simpledialog, messagebox, filedialog\n",
    "\n",
    "# Function to launch scripts\n",
    "def run_script(script_name):\n",
    "    try:\n",
    "        subprocess.Popen([\"python\", script_name])\n",
    "    except Exception as e:\n",
    "        status_label.configure(text=f\"Error: {e}\", text_color=\"red\")\n",
    "\n",
    "# Function to add a custom gesture action\n",
    "from tkinter import filedialog\n",
    "import ast\n",
    "\n",
    "def add_custom_gesture():\n",
    "    gesture_name = simpledialog.askstring(\"Custom Gesture\", \"Enter Gesture Name:\")\n",
    "    if not gesture_name:\n",
    "        return\n",
    "\n",
    "    action_type = simpledialog.askstring(\"Action Type\", \"Type 'key' to map a keyboard key or 'app' to open an application:\")\n",
    "    if action_type not in [\"key\", \"app\"]:\n",
    "        messagebox.showerror(\"Error\", \"Invalid action type. Use 'key' or 'app'.\")\n",
    "        return\n",
    "\n",
    "    if action_type == \"key\":\n",
    "        key = simpledialog.askstring(\"Key Mapping\", \"Enter the keyboard key to map:\")\n",
    "        if not key:\n",
    "            return\n",
    "        action_code = f\"lambda: pyautogui.press('{key}')\"\n",
    "    else:\n",
    "        app_path = filedialog.askopenfilename(title=\"Select Application\", filetypes=[(\"Executable Files\", \"*.exe;*.bat;*.cmd\"), (\"All Files\", \"*.*\")])\n",
    "        if not app_path:\n",
    "            return\n",
    "        action_code = f\"lambda: subprocess.Popen(r'{app_path}')\"\n",
    "\n",
    "    # Path to gesture_actions.py\n",
    "    file_path = \"gesture_actions.py\"\n",
    "\n",
    "    # Read existing content\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, \"r\") as file:\n",
    "            lines = file.readlines()\n",
    "    else:\n",
    "        lines = [\"import pyautogui\\n\", \"import subprocess\\n\", \"\\n\", \"GESTURE_ACTIONS = {\\n\"]\n",
    "\n",
    "    # Check if the gesture already exists\n",
    "    for line in lines:\n",
    "        if f'\"{gesture_name}\":' in line:\n",
    "            messagebox.showerror(\"Error\", f\"Gesture '{gesture_name}' already exists.\")\n",
    "            return\n",
    "\n",
    "    # Insert new gesture before the closing bracket\n",
    "    for i in range(len(lines) - 1, -1, -1):\n",
    "        if lines[i].strip() == \"}\":\n",
    "            lines.insert(i, f'    \"{gesture_name}\": {action_code},\\n')\n",
    "            break\n",
    "    else:\n",
    "        # If \"}\" is not found, assume the dictionary is empty or malformed\n",
    "        lines.append(f'    \"{gesture_name}\": {action_code},\\n')\n",
    "        lines.append(\"}\\n\")\n",
    "\n",
    "    # Write back updated content\n",
    "    with open(file_path, \"w\") as file:\n",
    "        file.writelines(lines)\n",
    "\n",
    "    messagebox.showinfo(\"Success\", f\"Gesture '{gesture_name}' saved successfully!\")\n",
    "\n",
    "# Initialize Modern UI\n",
    "ctk.set_appearance_mode(\"dark\")\n",
    "ctk.set_default_color_theme(\"blue\")\n",
    "\n",
    "root = ctk.CTk()\n",
    "root.title(\"ISL Recognition System\")\n",
    "root.geometry(\"500x400\")\n",
    "\n",
    "frame = ctk.CTkFrame(root)\n",
    "frame.pack(pady=20, padx=20, fill=\"both\", expand=True)\n",
    "\n",
    "ctk.CTkLabel(frame, text=\"Indian Sign Language System\", font=(\"Arial\", 20)).pack(pady=10)\n",
    "\n",
    "# Buttons for menu options\n",
    "buttons = [\n",
    "    (\"Record Gestures\", \"green\", \"record_gestures.py\"),\n",
    "    (\"Run Main ISL Recognition\", \"blue\", \"main.py\"),\n",
    "    (\"Custom Gesture Action\", \"purple\", add_custom_gesture),\n",
    "    (\"Settings\", \"orange\", \"settings.py\"),\n",
    "    (\"Exit\", \"red\", root.quit)\n",
    "]\n",
    "\n",
    "for text, color, script in buttons:\n",
    "    btn = ctk.CTkButton(frame, text=text, fg_color=color, command=lambda s=script: run_script(s) if isinstance(s, str) else s())\n",
    "    btn.pack(pady=5, padx=20, fill=\"x\")\n",
    "\n",
    "status_label = ctk.CTkLabel(frame, text=\"Select an option\", text_color=\"white\")\n",
    "status_label.pack(pady=10)\n",
    "\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
